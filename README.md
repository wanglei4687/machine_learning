# MACHINE LEARNING


## Presentaions

- [Andrew Ng: Opportunities in AI - 2023](https://www.youtube.com/watch?v=5p248yoa3oE)

## BOOK LIST

- Introduction to High-Dimensional Statistics (2021, Chapman and Hall_CRC) 
- Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems (2012)
- Prediction, Learning, and Games (2006, Cambridge University Press)
- Understanding Machine Learning_ From Theory to Algorithms (2014, CUP)

## Repo

- [stable diffusion v2](https://github.com/Stability-AI/stablediffusion)

## OPENE COURSE LIST

- [MIT 18.657 Mathematics Of Machine Learning](https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/)

## PAPER LIST

### Understanding the Main Architecture and Tasks

- [Neural Machine Translation by Jointly Learning to Align and Translate (2014)](https://arxiv.org/abs/1409.0473)
- [Attention Is All You Need (2017) ](https://arxiv.org/abs/1706.03762)
- [On Layer Normalization in the Transformer Architecture (2020)](https://arxiv.org/abs/2002.04745)
- [Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Neural Networks (1991)](https://www.semanticscholar.org/paper/Learning-to-Control-Fast-Weight-Memories%3A-An-to-Schmidhuber/bc22e87a26d020215afe91c751e5bdaddd8e4922)
- [Universal Language Model Fine-tuning for Text Classification (2018)](https://arxiv.org/abs/1801.06146)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)](https://arxiv.org/abs/1810.04805)
- [Improving Language Understanding by Generative Pre-Training (2018) by Radford and Narasimhan]( https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)
- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (2019) ](https://arxiv.org/abs/1910.13461)
- [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond (2023)](https://arxiv.org/abs/2304.13712)

### Scaling Laws and Improving Efficiency

- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (2022)](https://arxiv.org/abs/2205.14135)
- [Cramming: Training a Language Model on a Single GPU in One Day (2022)](https://arxiv.org/abs/2212.14034)
- [LoRA: Low-Rank Adaptation of Large Language Models (2021)](https://arxiv.org/abs/2106.09685)
- [Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning (2022)](https://arxiv.org/abs/2303.15647)
- [Scaling Language Models: Methods, Analysis & Insights from Training Gopher (2022) ](https://arxiv.org/abs/2112.11446)
- [Training Compute-Optimal Large Language Models (2022)](https://arxiv.org/abs/2203.15556)
- [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling (2023)](https://arxiv.org/abs/2304.01373)

### Alignment â€“ Steering Large Language Models to Intended Goals and Interests

- [Training Language Models to Follow Instructions with Human Feedback (2022) ](https://arxiv.org/abs/2203.02155)
- [Constitutional AI: Harmlessness from AI Feedback (2022)](https://arxiv.org/abs/2212.08073)
- [Self-Instruct: Aligning Language Model with Self Generated Instruction (2022)](https://arxiv.org/abs/2212.10560)

### Bonus: Introduction to Reinforcement Learning with Human Feedback (RLHF)

- [Asynchronous Methods for Deep Reinforcement Learning (2016) ](https://arxiv.org/abs/1602.01783)
- [Proximal Policy Optimization Algorithms (2017) ](https://arxiv.org/abs/1707.06347)
- [Fine-Tuning Language Models from Human Preferences (2020) ](https://arxiv.org/abs/1909.08593)
- [Learning to Summarize from Human Feedback (2022)](https://arxiv.org/abs/2009.01325)
- [Training Language Models to Follow Instructions with Human Feedback (2022)](https://arxiv.org/abs/2203.02155) 

## Presentaions